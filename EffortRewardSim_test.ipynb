{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "# If you have a local module called paramsgen (and need it), uncomment:\n",
    "# from paramsgen import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_efforts_with_model(subdf, params, fixed_sigma=10.0):\n",
    "    \"\"\"\n",
    "    subdf: participant's DataFrame with columns at least:\n",
    "       trial, threshold, reward\n",
    "       (chosen_effort is optional if you only want the model's behavior).\n",
    "    params = [alpha_r, alpha_e, kappa, beta]\n",
    "    fixed_sigma: float\n",
    "    \n",
    "    Returns a new DataFrame with columns:\n",
    "      trial, threshold, reward, predicted_effort\n",
    "      plus optional columns R_hat, E_hat if you want to store them.\n",
    "    \"\"\"\n",
    "\n",
    "    alpha_r, alpha_e, kappa, beta = params\n",
    "\n",
    "    # Initialize hidden states\n",
    "    R_hat = 3.0\n",
    "    E_hat = 50.0\n",
    "\n",
    "    # Sort by trial to ensure correct chronological order\n",
    "    subdf = subdf.sort_values('trial', ascending=True)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for i, row in subdf.iterrows():\n",
    "        threshold_t = row['threshold']\n",
    "        reward_t    = row['reward']\n",
    "\n",
    "        # Evaluate EU(e) for e=0..100\n",
    "        e_values = np.arange(101)\n",
    "        EU_list  = []\n",
    "        for e in e_values:\n",
    "            p_succ = norm.cdf((e - E_hat) / fixed_sigma)\n",
    "            SV     = R_hat - kappa * e\n",
    "            EU_list.append(SV * p_succ)\n",
    "\n",
    "        EU_array = np.array(EU_list)\n",
    "        # Softmax or Argmax approach:\n",
    "        maxEU = np.max(EU_array)\n",
    "        expEU = np.exp(beta * (EU_array - maxEU))\n",
    "        probs = expEU / np.sum(expEU)\n",
    "        predicted_e = np.random.choice(e_values, p=probs)\n",
    "        # or for deterministic:\n",
    "        # predicted_e = e_values[np.argmax(EU_array)]\n",
    "\n",
    "        # (optional) outcome logic\n",
    "        p_succ_obs = norm.cdf((predicted_e - E_hat)/fixed_sigma)\n",
    "        outcome = 1 if (np.random.rand() < p_succ_obs) else 0\n",
    "        received_reward = reward_t if outcome==1 else 0.0\n",
    "\n",
    "        # Update learners\n",
    "        R_hat += alpha_r*(received_reward - R_hat)\n",
    "        E_hat += alpha_e*(threshold_t - E_hat)\n",
    "        E_hat = np.clip(E_hat, 0, 100)\n",
    "\n",
    "        records.append({\n",
    "            'trial'           : row['trial'],\n",
    "            'threshold'       : threshold_t,\n",
    "            'reward'          : reward_t,\n",
    "            'predicted_effort': predicted_e,\n",
    "            'R_hat'           : R_hat,\n",
    "            'E_hat'           : E_hat\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_participant(big_dataframe, results_df, fixed_sigma=10.0):\n",
    "    for idx, row in results_df.iterrows():\n",
    "        pid     = row['id']\n",
    "        alpha_r = row['alpha_r']\n",
    "        alpha_e = row['alpha_e']\n",
    "        kappa   = row['kappa']\n",
    "        beta    = row['beta']\n",
    "\n",
    "        # Extract that participant's data from big_dataframe\n",
    "        subdf = big_dataframe[big_dataframe['id'] == pid].copy()\n",
    "        subdf.sort_values('trial', ascending=True, inplace=True)\n",
    "\n",
    "        # If the user data has 'chosen_effort', we can plot it\n",
    "        if 'chosen_effort' not in subdf.columns:\n",
    "            print(f\"No chosen_effort column for participant {pid}, skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        # Use best-fit params to generate predicted efforts\n",
    "        params = [alpha_r, alpha_e, kappa, beta]\n",
    "        pred_df = predict_efforts_with_model(subdf, params, fixed_sigma=fixed_sigma)\n",
    "\n",
    "        # Merge so we have trial, threshold, chosen_effort, predicted_effort all in one DataFrame\n",
    "        merged = pd.merge(subdf, pred_df, on='trial', how='left')\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(merged['trial'], merged['threshold'], label='Threshold', color='blue')\n",
    "        plt.plot(merged['trial'], merged['chosen_effort'], label='Actual Chosen Effort', color='red')\n",
    "        plt.plot(merged['trial'], merged['predicted_effort'], label='Predicted Effort', color='green')\n",
    "\n",
    "        plt.xlabel('Trial')\n",
    "        plt.ylabel('Effort / Threshold')\n",
    "        plt.title(f\"Participant {pid} (alpha_r={alpha_r:.2f}, alpha_e={alpha_e:.2f}, kappa={kappa:.3f}, beta={beta:.1f})\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded task_data_nobehav.csv (renamed to 'reward' and 'threshold').\n",
      "Reading CSV file: sub-66a8029007c122fe8086afcb_rewardeffortlearning_2024-09-06_11h17.22.698.csv\n",
      "Skipping file: sub-6547bf8012d4702680d55663_rewardeffortlearning_2024-09-06_10h30.03.678.log.gz\n",
      "Reading CSV file: sub-55bd8669fdf99b5bfc7d4cfc_rewardeffortlearning_2024-09-06_11h25.07.641.csv\n",
      "Reading CSV file: sub-63474e67a5fd298c6103c409_rewardeffortlearning_2024-09-06_10h50.53.039.csv\n",
      "Reading CSV file: sub-5cba9a8a214b1a0016ccf708_rewardeffortlearning_2024-09-06_08h21.17.302.csv\n",
      "Skipping file: sub-601b28841ca7bb2f8edaa293_rewardeffortlearning_2024-09-06_11h16.31.372.log.gz\n",
      "Skipping file: PARTICIPANT_RewardEffortLearning_pilot2_2024-09-06_11h56.00.479.log.gz\n",
      "Skipping file: .DS_Store\n",
      "Skipping file: PARTICIPANT_RewardEffortLearning_pilot2_2024-09-06_11h53.27.161.csv\n",
      "Skipping file: sub-66706a23c37c6099480f039d_rewardeffortlearning_2024-09-06_11h17.32.305.log.gz\n",
      "Skipping file: sub-600319e3a3b1a337cab57a3e_rewardeffortlearning_2024-09-06_11h19.10.675.log.gz\n",
      "Skipping file: sub-6333926e48c34a5d39353558_rewardeffortlearning_2024-09-06_11h16.35.146.log.gz\n",
      "Skipping file: sub-6676fab6295042ff191ead64_rewardeffortlearning_2024-09-06_11h15.26.211.log.gz\n",
      "Skipping file: sub-644f255ec4f58fe559261911_rewardeffortlearning_2024-09-06_11h20.22.385.log.gz\n",
      "Skipping file: PARTICIPANT_RewardEffortLearning_pilot2_2024-09-06_11h53.51.697.csv\n",
      "Reading CSV file: sub-644f255ec4f58fe559261911_rewardeffortlearning_2024-09-06_11h55.37.542.csv\n",
      "Skipping file: sub-55b237e6fdf99b19ea79d2f7_rewardeffortlearning_2024-09-05_17h05.47.845.log.gz\n",
      "Reading CSV file: sub-57864d24900cc80001d2de56_rewardeffortlearning_2024-09-06_11h15.53.897.csv\n",
      "Skipping file: PARTICIPANT_RewardEffortLearning_pilot2_2024-09-06_11h53.51.697.log.gz\n",
      "Skipping file: sub-60ca4cfb5c287146cad0695a_rewardeffortlearning_2024-09-06_11h15.49.510.log.gz\n",
      "Reading CSV file: sub-62728f0c68b9ecf571a2dbd4_rewardeffortlearning_2024-09-06_10h14.25.782.csv\n",
      "Reading CSV file: sub-66ca889632a6b5e4f4d0acf7_rewardeffortlearning_2024-09-06_11h16.02.149.csv\n",
      "Skipping file: big_dataframe.csv\n",
      "Reading CSV file: sub-6676fab6295042ff191ead64_rewardeffortlearning_2024-09-06_11h15.26.211.csv\n",
      "Reading CSV file: sub-6333926e48c34a5d39353558_rewardeffortlearning_2024-09-06_11h16.35.146.csv\n",
      "Reading CSV file: sub-60ca4cfb5c287146cad0695a_rewardeffortlearning_2024-09-06_11h15.49.510.csv\n",
      "Reading CSV file: sub-644f255ec4f58fe559261911_rewardeffortlearning_2024-09-06_11h20.22.385.csv\n",
      "Reading CSV file: sub-55bd8669fdf99b5bfc7d4cfc_rewardeffortlearning_2024-09-06_11h25.45.476.csv\n",
      "Reading CSV file: sub-5d74001d391b6600175f433b_rewardeffortlearning_2024-09-06_08h30.35.989.csv\n",
      "Skipping file: sub-6638e8aa3d1f38846080806a_rewardeffortlearning_2024-09-06_11h17.41.371.log.gz\n",
      "Skipping file: sub-55bd8669fdf99b5bfc7d4cfc_rewardeffortlearning_2024-09-06_11h25.45.476.log.gz\n",
      "Reading CSV file: sub-59ff47d47ecfc50001be0555_rewardeffortlearning_2024-09-06_10h17.05.655.csv\n",
      "Skipping file: sub-66ad113c81c9f9249fca2993_rewardeffortlearning_2024-09-06_10h35.09.587.log.gz\n",
      "Reading CSV file: sub-6638e8aa3d1f38846080806a_rewardeffortlearning_2024-09-06_11h17.41.371.csv\n",
      "Reading CSV file: sub-601b28841ca7bb2f8edaa293_rewardeffortlearning_2024-09-06_11h16.31.372.csv\n",
      "Skipping file: sub-5cba9a8a214b1a0016ccf708_rewardeffortlearning_2024-09-06_08h21.17.302.log.gz\n",
      "Skipping file: sub-66ca889632a6b5e4f4d0acf7_rewardeffortlearning_2024-09-06_11h16.02.149.log.gz\n",
      "Skipping file: PARTICIPANT_RewardEffortLearning_pilot2_2024-09-06_11h56.00.479.csv\n",
      "Reading CSV file: sub-644f255ec4f58fe559261911_rewardeffortlearning_2024-09-06_11h53.07.112.csv\n",
      "Reading CSV file: sub-6404132ea192589b51692c4a_rewardeffortlearning_2024-09-06_11h23.08.591.csv\n",
      "Reading CSV file: sub-55b237e6fdf99b19ea79d2f7_rewardeffortlearning_2024-09-05_17h05.47.845.csv\n",
      "Skipping file: PARTICIPANT_RewardEffortLearning_pilot2_2024-09-06_11h53.27.161.log.gz\n",
      "Skipping file: sub-66a8029007c122fe8086afcb_rewardeffortlearning_2024-09-06_11h17.22.698.log.gz\n",
      "Reading CSV file: sub-66706a23c37c6099480f039d_rewardeffortlearning_2024-09-06_11h17.32.305.csv\n",
      "Reading CSV file: sub-66ad113c81c9f9249fca2993_rewardeffortlearning_2024-09-06_10h35.09.587.csv\n",
      "Skipping file: sub-6404132ea192589b51692c4a_rewardeffortlearning_2024-09-06_11h23.08.591.log.gz\n",
      "Skipping file: sub-59ff47d47ecfc50001be0555_rewardeffortlearning_2024-09-06_10h17.05.655.log.gz\n",
      "Skipping file: task_data_nobehav.csv\n",
      "Skipping file: sub-62728f0c68b9ecf571a2dbd4_rewardeffortlearning_2024-09-06_10h14.25.782.log.gz\n",
      "Skipping file: sub-63474e67a5fd298c6103c409_rewardeffortlearning_2024-09-06_10h50.53.039.log.gz\n",
      "Reading CSV file: sub-600319e3a3b1a337cab57a3e_rewardeffortlearning_2024-09-06_11h19.10.675.csv\n",
      "Reading CSV file: sub-6547bf8012d4702680d55663_rewardeffortlearning_2024-09-06_10h30.03.678.csv\n",
      "Skipping file: sub-5d74001d391b6600175f433b_rewardeffortlearning_2024-09-06_08h30.35.989.log.gz\n",
      "\n",
      "Concatenated 24 participant DataFrames into one: 1670 rows total.\n",
      "\n",
      "Columns in big_dataframe: Index(['--', 'id', 'study_id', 'session_id', 'date', 'expName',\n",
      "       'psychopyVersion', 'OS', 'frameRate', 'reward', 'FeedbackText1',\n",
      "       'FeedbackText2', 'stimFile', 'threshold', 'rewRating',\n",
      "       'AnchorRewRating', 'AccuracyRewardRating', 'PossiblePointsSoFar',\n",
      "       'AccumulatedPointsSoFar', 'reward.response', 'reward.rt',\n",
      "       'login_reward.keys', 'login_reward.rt', 'login_reward.duration',\n",
      "       'effRating', 'AnchorEffRating', 'AccuracyEffortRating',\n",
      "       'slider.response', 'slider.rt', 'reactionTimes', 'reactionTimesData',\n",
      "       'averageReactionTimesData', 'rtPercentageData', 'maxTime', 'minTime',\n",
      "       'maxPosBar', 'minPosBar', 'Accum points main trials', 'chosen_effort',\n",
      "       'minimum RT on current trial', 'achieved', 'trials.thisRepN', 'trial',\n",
      "       'trials.thisN', 'trials.thisIndex', 'trials.ran'],\n",
      "      dtype='object')\n",
      "Sample rows:\n",
      "                                       --  id                  study_id  \\\n",
      "28  Please be patient as the study loads   1  66d895541319693e0a3a3141   \n",
      "29  Please be patient as the study loads   1  66d895541319693e0a3a3141   \n",
      "30  Please be patient as the study loads   1  66d895541319693e0a3a3141   \n",
      "31  Please be patient as the study loads   1  66d895541319693e0a3a3141   \n",
      "32  Please be patient as the study loads   1  66d895541319693e0a3a3141   \n",
      "\n",
      "                  session_id                     date  \\\n",
      "28  66db1c8f32641f2af068274d  2024-09-06_11h17.22.698   \n",
      "29  66db1c8f32641f2af068274d  2024-09-06_11h17.22.698   \n",
      "30  66db1c8f32641f2af068274d  2024-09-06_11h17.22.698   \n",
      "31  66db1c8f32641f2af068274d  2024-09-06_11h17.22.698   \n",
      "32  66db1c8f32641f2af068274d  2024-09-06_11h17.22.698   \n",
      "\n",
      "                         expName psychopyVersion     OS  frameRate  reward  \\\n",
      "28  RewardEffortLearning_pilot7b        2023.1.3  Win32  60.313631     2.0   \n",
      "29  RewardEffortLearning_pilot7b        2023.1.3  Win32  60.313631     2.0   \n",
      "30  RewardEffortLearning_pilot7b        2023.1.3  Win32  60.313631     0.0   \n",
      "31  RewardEffortLearning_pilot7b        2023.1.3  Win32  60.313631     0.0   \n",
      "32  RewardEffortLearning_pilot7b        2023.1.3  Win32  60.313631     1.0   \n",
      "\n",
      "    ... minPosBar Accum points main trials chosen_effort  \\\n",
      "28  ...      -0.5                      2.0     67.748807   \n",
      "29  ...      -0.5                      4.0     85.535071   \n",
      "30  ...      -0.5                      4.0     65.875657   \n",
      "31  ...      -0.5                      4.0     84.015750   \n",
      "32  ...      -0.5                      5.0     84.115184   \n",
      "\n",
      "    minimum RT on current trial  achieved  trials.thisRepN  trial  \\\n",
      "28                        0.019      True              0.0    0.0   \n",
      "29                        0.020      True              0.0    1.0   \n",
      "30                        0.021      True              0.0    2.0   \n",
      "31                        0.019      True              0.0    3.0   \n",
      "32                        0.023      True              0.0    4.0   \n",
      "\n",
      "    trials.thisN  trials.thisIndex  trials.ran  \n",
      "28           0.0               0.0         1.0  \n",
      "29           1.0               1.0         1.0  \n",
      "30           2.0               2.0         1.0  \n",
      "31           3.0               3.0         1.0  \n",
      "32           4.0               4.0         1.0  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "Saved 'big_dataframe.csv'.\n",
      "\n",
      "Parameter Fit Results:\n",
      "     id  alpha_r  alpha_e  kappa      beta          nLL\n",
      "0    1      0.3      0.3   0.05  1.000000  1657.861267\n",
      "1    3      0.3      0.3   0.05  1.000000  1657.861267\n",
      "2    4      0.3      0.3   0.05  1.000000  1657.861267\n",
      "3    7      0.3      0.3   0.05  1.000000  1657.861267\n",
      "4    8      0.3      0.3   0.05  1.000000  1657.861267\n",
      "5    9      0.3      0.3   0.05  1.000000  1657.861267\n",
      "6   10      0.3      0.3   0.05  1.000000  1657.861267\n",
      "7   11      0.3      0.3   0.05  1.000000  1657.861267\n",
      "8    5      0.3      0.3   0.05  1.000000  1634.588901\n",
      "9    2      0.3      0.3   0.05  1.000000  1657.861267\n",
      "10  12      0.3      0.3   0.05  1.000000  1657.861267\n",
      "11  13      0.3      0.3   0.05  1.000000  1657.861267\n",
      "12  14      0.3      0.3   0.05  1.000000  1657.861267\n",
      "13  15      0.3      0.3   0.05  1.000000  1657.861267\n",
      "14  16      0.3      0.3   0.05  1.000000  1657.861267\n",
      "15  17      0.3      0.3   0.05  1.000003  1634.367994\n",
      "16  18      0.3      0.3   0.05  1.000000  1657.861267\n",
      "17  19      0.3      0.3   0.05  1.000000  1657.861267\n",
      "18  20      0.3      0.3   0.05  1.000000  1657.861267\n",
      "19  21      0.3      0.3   0.05  1.000003  1372.972047\n",
      "Saved parameter fit results to fit_results_fixedSigma_20250217_110652.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'threshold'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 431\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Run main automatically (comment out if you prefer to call main() manually)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 431\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 416\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    414\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(out_filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved parameter fit results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 416\u001b[0m \u001b[43mplot_per_participant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbig_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Plot parameter distributions\u001b[39;00m\n\u001b[1;32m    419\u001b[0m params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_r\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_e\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mplot_per_participant\u001b[0;34m(big_dataframe, results_df, fixed_sigma)\u001b[0m\n\u001b[1;32m     23\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(subdf, pred_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m---> 27\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mmerged\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThreshold\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m'\u001b[39m], merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen_effort\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Chosen Effort\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m'\u001b[39m], merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_effort\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Effort\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'threshold'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# 1) READING AND PREPARING THE DATA\n",
    "###############################################################################\n",
    "def read_csv_files(directory):\n",
    "    \"\"\"\n",
    "    Reads all 'sub*.csv' files in the specified directory and returns a list of DataFrames.\n",
    "    Skips any files that do not match that pattern.\n",
    "    \"\"\"\n",
    "    csv_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"sub\") and filename.endswith(\".csv\"):\n",
    "            print(f\"Reading CSV file: {filename}\")\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            csv_files.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping file: {filename}\")\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "def map_prolific_ids(dataframe):\n",
    "    \"\"\"\n",
    "    Maps each unique prolific_id to an integer in [1..100].\n",
    "    Returns the modified DataFrame and the mapping dictionary.\n",
    "    \"\"\"\n",
    "    unique_ids = dataframe['prolific_id'].unique()\n",
    "    id_mapping = {pid: (i % 100) + 1 for i, pid in enumerate(unique_ids)}\n",
    "    dataframe['prolific_id'] = dataframe['prolific_id'].map(id_mapping)\n",
    "    return dataframe, id_mapping\n",
    "\n",
    "\n",
    "def normalize_column_to_0_100(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Rescales the indicated column to [0..100].\n",
    "    \"\"\"\n",
    "    min_val = dataframe[column_name].min()\n",
    "    max_val = dataframe[column_name].max()\n",
    "    scaled = 100.0 * (dataframe[column_name] - min_val) / (max_val - min_val)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) LOAD THE DATA\n",
    "###############################################################################\n",
    "current_directory = os.getcwd()\n",
    "data_directory = os.path.join(current_directory, 'data')\n",
    "\n",
    "# 2a) Load a special \"task_data_nobehav.csv\" if it exists\n",
    "task_data_path = os.path.join(data_directory, 'task_data_nobehav.csv')\n",
    "task_data = None\n",
    "if os.path.exists(task_data_path):\n",
    "    task_data = pd.read_csv(task_data_path)\n",
    "    # Rename columns to match our modeling approach\n",
    "    # (If you need 'reward' and 'threshold' in your simulation, do so here)\n",
    "    task_data.rename(columns={'Points': 'reward', 'effLevel': 'threshold'}, inplace=True)\n",
    "    # Keep only relevant columns\n",
    "    task_data = task_data[['reward', 'threshold']]\n",
    "    print(\"Loaded task_data_nobehav.csv (renamed to 'reward' and 'threshold').\")\n",
    "\n",
    "# 2b) Load participant CSV files\n",
    "dataframes = read_csv_files(data_directory)\n",
    "\n",
    "# Ensure task_data is not included accidentally in the dataframes list\n",
    "if task_data is not None:\n",
    "    dataframes = [df for df in dataframes if not df.equals(task_data)]\n",
    "\n",
    "# Concatenate all participant DataFrames into one big DataFrame\n",
    "big_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"\\nConcatenated {len(dataframes)} participant DataFrames into one: {len(big_dataframe)} rows total.\")\n",
    "\n",
    "# 2c) Map prolific IDs to numeric\n",
    "big_dataframe, id_mapping_df = map_prolific_ids(big_dataframe)\n",
    "\n",
    "# 2d) Drop rows where threshold column (effLevel) is NaN\n",
    "if 'effLevel' in big_dataframe.columns:\n",
    "    big_dataframe = big_dataframe.dropna(subset=['effLevel'])\n",
    "\n",
    "# Also drop rows where trials.thisN or trials.thisTrialN is NaN\n",
    "if 'trials.thisN' in big_dataframe.columns:\n",
    "    big_dataframe = big_dataframe.dropna(subset=['trials.thisN'])\n",
    "if 'trials.thisTrialN' in big_dataframe.columns:\n",
    "    big_dataframe = big_dataframe.dropna(subset=['trials.thisTrialN'])\n",
    "\n",
    "# Drop empty columns\n",
    "big_dataframe = big_dataframe.dropna(axis=1, how='all')\n",
    "\n",
    "# Rename columns to consistent names\n",
    "rename_map = {\n",
    "    'Points': 'reward',\n",
    "    'effLevel': 'threshold',\n",
    "    'prolific_id': 'id',\n",
    "    'percentage of bar reached': 'chosen_effort',\n",
    "    'trials.thisTrialN': 'trial'  # or 'trials.thisN'\n",
    "}\n",
    "for old_name, new_name in rename_map.items():\n",
    "    if old_name in big_dataframe.columns:\n",
    "        big_dataframe.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "# Debug print\n",
    "print(\"\\nColumns in big_dataframe:\", big_dataframe.columns)\n",
    "print(\"Sample rows:\\n\", big_dataframe.head())\n",
    "\n",
    "# Save the big DataFrame for reference\n",
    "big_dataframe.to_csv(os.path.join(data_directory, 'big_dataframe.csv'), index=False)\n",
    "print(\"\\nSaved 'big_dataframe.csv'.\")\n",
    "\n",
    "###############################################################################\n",
    "# 3) OPTIONAL: CLASSES FOR SIMULATION (IF NEEDED)\n",
    "###############################################################################\n",
    "class RewardLearner:\n",
    "    \"\"\"\n",
    "    Simple Rescorla-Wagner:\n",
    "      R_hat[t] = R_hat[t-1] + alpha_r * (R_t - R_hat[t-1])\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha_r=0.1, init_reward=3.5, sigma_r=0.7):\n",
    "        self.alpha_r = alpha_r\n",
    "        self.R_hat = init_reward\n",
    "        self.sigma_r = sigma_r\n",
    "\n",
    "    def update(self, received_reward):\n",
    "        delta_r = received_reward - self.R_hat\n",
    "        self.R_hat += self.alpha_r * delta_r\n",
    "        return self.R_hat\n",
    "\n",
    "    def belief_distribution(self):\n",
    "        return norm(loc=self.R_hat, scale=self.sigma_r)\n",
    "\n",
    "\n",
    "class EffortLearner:\n",
    "    \"\"\"\n",
    "    Direct threshold feedback:\n",
    "      E_hat[t+1] = E_hat[t] + alpha_e * (threshold[t] - E_hat[t])\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha_e=0.1, init_effort=50.0):\n",
    "        self.alpha_e = alpha_e\n",
    "        self.E_hat = init_effort\n",
    "\n",
    "    def update(self, true_threshold):\n",
    "        delta_e = true_threshold - self.E_hat\n",
    "        self.E_hat += self.alpha_e * delta_e\n",
    "        self.E_hat = np.clip(self.E_hat, 0, 100)\n",
    "        return self.E_hat\n",
    "\n",
    "\n",
    "class EffortDiscounter:\n",
    "    \"\"\"\n",
    "    We search effort e in [0..100], compute:\n",
    "      p_success(e) = norm.cdf((e - E_hat)/sigma)\n",
    "      SV(e) = R_hat - kappa*e\n",
    "      EU(e) = SV(e)*p_success(e)\n",
    "    Then either argmax or softmax policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, kappa=0.05, sigma=5.0, beta=5.0, policy='argmax'):\n",
    "        self.kappa = kappa\n",
    "        self.sigma = sigma\n",
    "        self.beta = beta\n",
    "        self.policy = policy\n",
    "\n",
    "    def choose_effort(self, R_hat, E_hat):\n",
    "        e_values = np.arange(101)  # integer efforts from 0..100\n",
    "        EU_values = []\n",
    "        for e in e_values:\n",
    "            p_success = norm.cdf((e - E_hat)/self.sigma)\n",
    "            SV = R_hat - self.kappa * e\n",
    "            EU_values.append(SV * p_success)\n",
    "\n",
    "        EU_values = np.array(EU_values)\n",
    "        if self.policy == 'argmax':\n",
    "            chosen_idx = np.argmax(EU_values)\n",
    "            return e_values[chosen_idx]\n",
    "        else:\n",
    "            # softmax\n",
    "            max_eu = np.max(EU_values)\n",
    "            exp_vals = np.exp(self.beta * (EU_values - max_eu))\n",
    "            probs = exp_vals / np.sum(exp_vals)\n",
    "            return np.random.choice(e_values, p=probs)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) SIMULATION FUNCTION (OPTIONAL)\n",
    "###############################################################################\n",
    "def simulate_experiment(num_trials=60,\n",
    "                        reward_learner_params=None,\n",
    "                        effort_learner_params=None,\n",
    "                        discounter_params=None,\n",
    "                        data=None,\n",
    "                        ignore_zero_reward=True,\n",
    "                        normalize_reward=False,\n",
    "                        normalize_effort=False):\n",
    "    \"\"\"\n",
    "    Example simulation to show how you'd step through an experiment.\n",
    "    This is optional if you already have real data.\n",
    "    \"\"\"\n",
    "    if reward_learner_params is None:\n",
    "        reward_learner_params = {}\n",
    "    if effort_learner_params is None:\n",
    "        effort_learner_params = {}\n",
    "    if discounter_params is None:\n",
    "        discounter_params = {}\n",
    "\n",
    "    if normalize_reward and (data is not None):\n",
    "        if ignore_zero_reward:\n",
    "            data['reward'] = data['reward'].replace(0, np.nan)\n",
    "        min_r = data['reward'].min()\n",
    "        max_r = data['reward'].max()\n",
    "        data['reward'] = (data['reward'] - min_r) / (max_r - min_r)\n",
    "\n",
    "    if normalize_effort and (data is not None):\n",
    "        data['threshold'] = normalize_column_to_0_100(data, 'threshold')\n",
    "\n",
    "    # Initialize model classes\n",
    "    RL = RewardLearner(**reward_learner_params)\n",
    "    EL = EffortLearner(**effort_learner_params)\n",
    "    discounter = EffortDiscounter(**discounter_params)\n",
    "\n",
    "    results = {\n",
    "        'trial': [],\n",
    "        'threshold': [],\n",
    "        'reward': [],\n",
    "        'chosen_effort': [],\n",
    "        'outcome': [],\n",
    "        'R_hat': [],\n",
    "        'E_hat': []\n",
    "    }\n",
    "\n",
    "    if data is None:\n",
    "        print(\"No data provided to simulate_experiment. Exiting.\")\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    actual_num_trials = min(num_trials, len(data))\n",
    "    for t in range(actual_num_trials):\n",
    "        threshold_t = data.iloc[t]['threshold']\n",
    "        reward_t = data.iloc[t]['reward']\n",
    "\n",
    "        if ignore_zero_reward and reward_t == 0:\n",
    "            print(f\"Trial {t+1}: Skipping zero-reward trial\")\n",
    "            continue\n",
    "\n",
    "        # Current estimates\n",
    "        R_hat_t = RL.R_hat\n",
    "        E_hat_t = EL.E_hat\n",
    "\n",
    "        chosen_e = discounter.choose_effort(R_hat_t, E_hat_t)\n",
    "        p_success = norm.cdf((chosen_e - E_hat_t) / discounter.sigma)\n",
    "        outcome = np.random.binomial(1, p_success)\n",
    "        received_reward = reward_t if outcome == 1 else 0.0\n",
    "\n",
    "        RL.update(received_reward)\n",
    "        EL.update(threshold_t)\n",
    "\n",
    "        results['trial'].append(t+1)\n",
    "        results['threshold'].append(threshold_t)\n",
    "        results['reward'].append(reward_t)\n",
    "        results['chosen_effort'].append(chosen_e)\n",
    "        results['outcome'].append(outcome)\n",
    "        results['R_hat'].append(RL.R_hat)\n",
    "        results['E_hat'].append(EL.E_hat)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) PARAMETER RECOVERY / FITTING\n",
    "###############################################################################\n",
    "def model_nll(params, subdf, fixed_sigma=10.0):\n",
    "    \"\"\"\n",
    "    Computes the negative log-likelihood under the effortâ€“reward model\n",
    "    for one participant's data, given parameters.\n",
    "\n",
    "    params = [alpha_r, alpha_e, kappa, beta]\n",
    "      alpha_r : reward learning rate in [0..1]\n",
    "      alpha_e : effort learning rate in [0..1]\n",
    "      kappa   : cost rate per unit effort in [0..(some range)]\n",
    "      beta    : inverse temperature for softmax\n",
    "\n",
    "    subdf: the participant's data (one ID), containing columns:\n",
    "           'trial', 'threshold', 'reward', 'chosen_effort', (optionally 'outcome')\n",
    "\n",
    "    fixed_sigma: we keep sigma constant for all participants in this example.\n",
    "    Returns: nLL (float) = negative log-likelihood\n",
    "    \"\"\"\n",
    "    alpha_r, alpha_e, kappa, beta = params\n",
    "    sigma = fixed_sigma\n",
    "\n",
    "    # Initialize hidden states\n",
    "    R_hat = 3.0   # near midpoint if reward ~1..7\n",
    "    E_hat = 50.0  # near midpoint if threshold ~0..100\n",
    "\n",
    "    # Sort by trial to ensure correct chronological order\n",
    "    subdf = subdf.sort_values('trial', ascending=True)\n",
    "\n",
    "    nLL = 0.0\n",
    "    eps = 1e-12\n",
    "\n",
    "    for _, row in subdf.iterrows():\n",
    "        threshold_t = row['threshold']\n",
    "        reward_t = row['reward']\n",
    "        chosenEff_t = row['chosen_effort']\n",
    "\n",
    "        # All possible efforts in [0..100]\n",
    "        e_values = np.arange(101)\n",
    "        EU = np.zeros_like(e_values, dtype=float)\n",
    "\n",
    "        for iE, e_test in enumerate(e_values):\n",
    "            p_succ = norm.cdf((e_test - E_hat) / sigma)\n",
    "            SV = R_hat - kappa * e_test\n",
    "            EU[iE] = SV * p_succ\n",
    "\n",
    "        # Softmax over EU\n",
    "        EU_max = np.max(EU)\n",
    "        expEU = np.exp(beta * (EU - EU_max))\n",
    "        p_e = expEU / (np.sum(expEU) + eps)\n",
    "\n",
    "        # Probability of the observed chosen effort\n",
    "        if chosenEff_t in e_values:\n",
    "            p_chosen = p_e[int(chosenEff_t)]\n",
    "        else:\n",
    "            # If for some reason chosenEff_t isn't an integer in [0..100], clamp\n",
    "            p_chosen = eps\n",
    "\n",
    "        nLL -= np.log(max(p_chosen, eps))\n",
    "\n",
    "        # Check if an actual outcome is in the data\n",
    "        if 'outcome' in subdf.columns:\n",
    "            outcome = row['outcome']\n",
    "        else:\n",
    "            # Or simulate outcome from the model\n",
    "            p_succ_obs = norm.cdf((chosenEff_t - E_hat) / sigma)\n",
    "            outcome = 1 if (np.random.rand() < p_succ_obs) else 0\n",
    "\n",
    "        # The participant receives 'reward_t' only if outcome==1\n",
    "        received_reward = reward_t if outcome == 1 else 0.0\n",
    "\n",
    "        # Update the learners\n",
    "        delta_r = received_reward - R_hat\n",
    "        R_hat = R_hat + alpha_r * delta_r\n",
    "\n",
    "        delta_e = threshold_t - E_hat\n",
    "        E_hat = E_hat + alpha_e * delta_e\n",
    "        E_hat = np.clip(E_hat, 0, 100)\n",
    "\n",
    "    return nLL\n",
    "\n",
    "\n",
    "def fit_participant(subdf, init_params=None, bounds=None, fixed_sigma=10.0):\n",
    "    \"\"\"\n",
    "    Minimizes negative log-likelihood for one participant with a fixed sigma.\n",
    "    subdf: DataFrame with that participant's data\n",
    "    init_params: initial guess [alpha_r, alpha_e, kappa, beta]\n",
    "    bounds: list of (min, max) for each parameter\n",
    "    fixed_sigma: constant sigma used in model_nll\n",
    "    \"\"\"\n",
    "    if init_params is None:\n",
    "        init_params = [0.3, 0.3, 0.05, 1.0]  # example guess\n",
    "    if bounds is None:\n",
    "        # alpha_r in [0,1], alpha_e in [0,1], kappa in [0,1], beta in [0,10]\n",
    "        bounds = [(0,1), (0,1), (0,1), (0,10)]\n",
    "\n",
    "    # Objective function for minimization\n",
    "    def objective(x):\n",
    "        return model_nll(x, subdf, fixed_sigma=fixed_sigma)\n",
    "\n",
    "    res = minimize(\n",
    "        fun=objective,\n",
    "        x0=init_params,\n",
    "        bounds=bounds,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxfun': 10000, 'disp': False}\n",
    "    )\n",
    "    return res.x, res.fun\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) MAIN: EXAMPLE PARAMETER RECOVERY\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Use the big_dataframe loaded at the top (contains all participants).\n",
    "    df = big_dataframe\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    # We'll store the fit results here\n",
    "    results_list = []\n",
    "\n",
    "    # We'll pick a fixed sigma for all participants for now\n",
    "    fixed_sigma_value = 10.0  # or e.g. 5.0, or 0.1, etc.\n",
    "\n",
    "    for pid in unique_ids:\n",
    "        subdf = df[df['id'] == pid].copy()\n",
    "        \n",
    "        # Fit the participant\n",
    "        best_params, best_nll = fit_participant(\n",
    "            subdf=subdf,\n",
    "            init_params=[0.3, 0.3, 0.05, 1.0],\n",
    "            bounds=[(0,1),(0,1),(0,1),(0,10)],\n",
    "            fixed_sigma=fixed_sigma_value\n",
    "        )\n",
    "        \n",
    "        alpha_r, alpha_e, kappa, beta = best_params\n",
    "        results_list.append({\n",
    "            'id': pid,\n",
    "            'alpha_r': alpha_r,\n",
    "            'alpha_e': alpha_e,\n",
    "            'kappa': kappa,\n",
    "            'beta': beta,\n",
    "            'nLL': best_nll\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame and save\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    print(\"\\nParameter Fit Results:\\n\", results_df)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    out_filename = f\"fit_results_fixedSigma_{timestamp}.csv\"\n",
    "    results_df.to_csv(out_filename, index=False)\n",
    "    print(f\"Saved parameter fit results to {out_filename}\")\n",
    "    plot_per_participant(big_dataframe, results_df, fixed_sigma=10.0)\n",
    "\n",
    "    # Plot parameter distributions\n",
    "    params = ['alpha_r','alpha_e','kappa','beta']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, p in enumerate(params, start=1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.histplot(data=results_df, x=p, kde=True)\n",
    "        plt.title(f\"Distribution of {p}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run main automatically (comment out if you prefer to call main() manually)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
